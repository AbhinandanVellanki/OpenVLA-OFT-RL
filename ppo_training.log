/home/abhi/miniconda3/envs/oft_rl/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import parse_version
[robosuite WARNING] No private macro file found! (__init__.py:7)
[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)
[robosuite WARNING] To setup, run: python /home/abhi/miniconda3/envs/oft_rl/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Using LIBERO constants:
  NUM_ACTIONS_CHUNK = 8
  ACTION_DIM = 7
  PROPRIO_DIM = 8
  ACTION_PROPRIO_NORMALIZATION_TYPE = bounds_q99
If needed, manually set the correct constants in `prismatic/vla/constants.py`!
âš ï¸  Warning: training_device (cuda:1) differs from device (cuda:0)
   This is advanced dual-GPU mode - ensure tensors are moved correctly!
Single-GPU Setup: All components on cuda:0
Initializing OpenVLA actor...
ðŸ”§ Single-GPU Setup:
   All components on cuda:0 (requires ~14.2GB total)
Loading pretrained VLA policy...
âœ“ Using local model from: /home/abhi/Documents/Deep-RL/OpenVLA-OFT-RL/vla_oft/openvla-7b-oft-finetuned-libero-spatial
âœ“ Updated /home/abhi/Documents/Deep-RL/OpenVLA-OFT-RL/vla_oft/openvla-7b-oft-finetuned-libero-spatial/config.json to use local prismatic code
âœ“ Using Flash Attention 2 (flash_attn version: 2.7.3)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  9.25it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 11.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.63it/s]
wandb: Currently logged in as: abhivel (deeprl_ais). Use `wandb login --relogin` to force relogin
/home/abhi/miniconda3/envs/oft_rl/lib/python3.10/site-packages/wandb/util.py:249: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.
  scope.user = {"email": s.email}
/home/abhi/miniconda3/envs/oft_rl/lib/python3.10/site-packages/wandb/util.py:249: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.
  scope.user = {"email": s.email}
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /home/abhi/Documents/Deep-RL/OpenVLA-OFT-RL/wandb/run-20251129_220751-euzrsrii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run libero_spatial_tasks0_PPO_20251129_220750
wandb: â­ï¸ View project at https://wandb.ai/deeprl_ais/OFT_RL
wandb: ðŸš€ View run at https://wandb.ai/deeprl_ais/OFT_RL/runs/euzrsrii
âœ“ Model loaded with attention: LlamaFlashAttention2
âœ“ Loaded dataset statistics from: /home/abhi/Documents/Deep-RL/OpenVLA-OFT-RL/vla_oft/openvla-7b-oft-finetuned-libero-spatial/dataset_statistics.json
âœ“ Loaded proprio projector from: /home/abhi/Documents/Deep-RL/OpenVLA-OFT-RL/vla_oft/openvla-7b-oft-finetuned-libero-spatial/proprio_projector--150000_checkpoint.pt
L1 regression action head disabled (using tokenized actions)
Enabled gradient checkpointing for memory efficiency
Initializing action tokenizer...
  ActionTokenizer(vocab_size=32000, n_bins=256, range=[-1.0, 1.0], tokens=[31744, 32000])

======================================================================
Applying LoRA Adapters to VLA Model
======================================================================
trainable params: 110,828,288 || all params: 7,652,065,472 || trainable%: 1.4483
LoRA Configuration:
  - Rank (r): 32
  - Alpha (Î±): 16
  - Dropout: 0.0
  - Target: all-linear layers
======================================================================

Initialized OpenVLA-PPO (GRPO - Value-Free Advantages)
  - Mode: Single-task
  - Num envs: 1
  - Actor trainable params: 127,646,464
  - Action prediction: Tokenized (PPO mode)
  - Action bins: 256
  - L1 regression head: Not loaded
  - Rollout temperature: 1.6
  - Clip ratios: high=0.28, low=0.2
  - GRPO gamma: 1.0
  - KL coefficient: 0.0
  - Device mode: Single-GPU
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

======================================================================
Starting PPO Training
======================================================================
Mode: Single-task
Task: pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate
Language: pick up the black bowl between the plate and the ramekin and place it on the plate
Total timesteps: 100000
======================================================================

[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

======================================================================
Running initial validation to establish baseline...
======================================================================
Validation:   0%|                                                                    | 0/10 episodesValidation:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                             | 1/10 episodesValidation:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 2/10 episodesValidation:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/10 episodes